import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import google.generativeai as genai

# --- [1. AI ì„¤ì •] ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    HAS_AI = True
except:
    HAS_AI = False

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/'
        }

    # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (AI ìš”ì•½ìš©)
    def get_article_body(self, url):
        if "n.news.naver.com" not in url: return ""
        try:
            res = self.scraper.get(url, headers=self.headers, timeout=5)
            soup = BeautifulSoup(res.content, 'html.parser')
            content = soup.select_one("#newsct_article") or soup.select_one("#articleBodyContents")
            return content.get_text(strip=True)[:2000] if content else ""
        except: return ""

    # AI ìš”ì•½ ì‹¤í–‰
    def summarize_ai(self, title, body, keyword):
        if not HAS_AI: return "API í‚¤ ì„¤ì • í›„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì½ê³  '{keyword}' ì—…ë¬´ ê´€ì ì—ì„œ í•µì‹¬ì„ 1ì¤„ë¡œ ìš”ì•½í•´ì¤˜.\në³¸ë¬¸: {body if body else title}"
            return model.generate_content(prompt).text.strip()
        except: return "ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def fetch_news(self, start_datetime, end_datetime, keyword):
        ds, de = start_datetime.strftime("%Y.%m.%d"), end_datetime.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_datetime.strftime('%Y%m%d')}to{end_datetime.strftime('%Y%m%d')}"
        all_results, seen_links = [], set()
        
        query = f'"{keyword}"'
        for page in range(1, 4):
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&start={start_index}"
            try:
                response = self.scraper.get(url, headers=self.headers, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]')
                if not items: break

                for t_tag in items:
                    title, link = t_tag.get_text(strip=True), t_tag.get('href')
                    if link in seen_links: continue
                    
                    # [íŒŒì‹± í•µì‹¬] ì¹´ë“œ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (SDS ë””ìì¸ ëŒ€ì‘)
                    card = t_tag.find_parent('div', class_=lambda c: c and ('api_subject_bx' in c or 'sds-comps' in c))
                    press_name, date_text, is_naver = "ì•Œ ìˆ˜ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "n.news.naver.com" in link
                    
                    if card:
                        # 1. ë„¤ì´ë²„ ì¸ë§í¬ ìš°ì„  íƒìƒ‰
                        naver_btn = card.select_one('a[href*="n.news.naver.com"]')
                        if naver_btn: link, is_naver = naver_btn.get('href'), True
                        
                        # 2. ì–¸ë¡ ì‚¬ ì¶”ì¶œ (ì •ë°€ í´ë˜ìŠ¤ íƒ€ê²ŸíŒ…)
                        press_el = card.select_one(".sds-comps-profile-info-title-text, .press_name, .info.press")
                        if press_el: press_name = press_el.get_text(strip=True)
                        
                        # 3. ì‹œê°„ ì¶”ì¶œ (subtexts ì˜ì—­ ìˆœíšŒ)
                        subtext_area = card.select_one(".sds-comps-profile-info-subtexts, .news_info")
                        if subtext_area:
                            for txt in subtext_area.stripped_strings:
                                if ('ì „' in txt and len(txt) < 15) or ('.' in txt and len(txt) < 15 and txt[0].isdigit()):
                                    date_text = txt; break

                    seen_links.add(link)
                    all_results.append({'title': title, 'link': link, 'press': press_name, 'time': date_text, 'is_naver': is_naver})
                time.sleep(0.3)
            except: break
        return all_results

# --- [3. Streamlit UI] ---
st.set_page_config(page_title="ì„œìš¸êµí†µê³µì‚¬ ìŠ¤í¬ë©", layout="wide")

# CSS: ë²„íŠ¼ 3ê°œ ë™ì¼ ìŠ¤íƒ€ì¼ ë° 1ì¤„ ë°°ì¹˜
st.markdown("""
    <style>
    .stButton > button, .stLinkButton > a {
        display: inline-flex !important; align-items: center !important; justify-content: center !important;
        width: 100% !important; height: 38px !important; background-color: #ffffff !important;
        color: #31333F !important; border: 1px solid #d1d5db !important; border-radius: 8px !important;
        font-size: 13px !important; font-weight: 600 !important; margin: 0 !important;
        text-decoration: none !important;
    }
    .stButton > button:hover, .stLinkButton > a:hover {
        border-color: #007bff !important; color: #007bff !important; background-color: #f0f7ff !important;
    }
    .news-card {
        background: white; padding: 14px; border-radius: 12px; border-left: 6px solid #007bff;
        margin-bottom: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    .ai-box { background-color: #f3f0ff; color: #553c9a; padding: 10px; border-radius: 8px; font-size: 13px; margin-top: 8px; border-left: 4px solid #9f7aea; }
    </style>
    """, unsafe_allow_html=True)

if 'scrap_list' not in st.session_state: st.session_state.scrap_list = []
if 'search_results' not in st.session_state: st.session_state.search_results = []
if 'summaries' not in st.session_state: st.session_state.summaries = {}

st.title("ğŸš‡ ë‰´ìŠ¤ ìŠ¤í¬ë© (Mobile)")

# 1. ìƒë‹¨ ìŠ¤í¬ë© ëª©ë¡ (ê°€ë³€í˜• ë†’ì´)
st.subheader("ğŸ“‹ ì‹¤ì‹œê°„ ìŠ¤í¬ë© ëª©ë¡")
if st.session_state.scrap_list:
    final_text = "".join(st.session_state.scrap_list)
    dynamic_h = min(max(150, len(st.session_state.scrap_list) * 55), 450)
    st.text_area("ë‚´ìš© ë³µì‚¬ìš©", value=final_text, height=dynamic_h)
    if st.button("ğŸ—‘ï¸ ì „ì²´ ë¹„ìš°ê¸°"): st.session_state.scrap_list = []; st.rerun()
else: st.info("ê¸°ì‚¬ë¥¼ ì¶”ê°€í•˜ë©´ ì—¬ê¸°ì— ë‹´ê¹ë‹ˆë‹¤.")

st.divider()

# 2. ê²€ìƒ‰ ì¡°ê±´
with st.expander("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •", expanded=True):
    keyword = st.text_input("í•„ìˆ˜ í‚¤ì›Œë“œ", value="ì„œìš¸êµí†µê³µì‚¬")
    c1, c2 = st.columns(2)
    with c1: start_d = st.date_input("ì‹œì‘", datetime.date.today() - datetime.timedelta(days=1))
    with c2: end_d = st.date_input("ì¢…ë£Œ", datetime.date.today())
    filter_opt = st.radio("í•„í„°", ["ë„¤ì´ë²„ ê¸°ì‚¬", "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬", "ëª¨ë‘ ë³´ê¸°"], index=0, horizontal=True)

if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰", type="primary"):
    sc = NewsScraper()
    with st.spinner('ë‰´ìŠ¤ë¥¼ ì½ì–´ì˜¤ëŠ” ì¤‘...'):
        st.session_state.search_results = sc.fetch_news(start_d, end_d, keyword)

# 3. ê²°ê³¼ ì¶œë ¥
if st.session_state.search_results:
    res_list = st.session_state.search_results
    if filter_opt == "ë„¤ì´ë²„ ê¸°ì‚¬": res_list = [r for r in res_list if r['is_naver']]
    elif filter_opt == "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬": res_list = [r for r in res_list if not r['is_naver']]

    st.subheader(f"âœ… ê²°ê³¼: {len(res_list)}ê±´")
    for i, res in enumerate(res_list):
        with st.container():
            st.markdown(f"""
            <div class="news-card">
                <strong>[{res['press']}]</strong> {res['title']}<br>
                <small style="color:gray;">{res['time']} {'(ë„¤ì´ë²„)' if res['is_naver'] else ''}</small>
            </div>
            """, unsafe_allow_html=True)
            
            # AI ìš”ì•½ ë°•ìŠ¤ (ìš”ì•½ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ í‘œì‹œ)
            if res['link'] in st.session_state.summaries:
                st.markdown(f'<div class="ai-box">âœ¨ <b>AI ìš”ì•½:</b> {st.session_state.summaries[res["link"]]}</div>', unsafe_allow_html=True)

            # [3ê°œ ë²„íŠ¼ í•œ ì¤„ ë°°ì¹˜]
            b1, b2, b3 = st.columns(3)
            with b1:
                if st.button("âœ¨ ìš”ì•½", key=f"sum_{i}"):
                    sc = NewsScraper()
                    with st.spinner('ë¶„ì„ ì¤‘...'):
                        body = sc.get_article_body(res['link'])
                        st.session_state.summaries[res['link']] = sc.summarize_ai(res['title'], body, keyword)
                        st.rerun()
            with b2:
                st.link_button("ğŸ”— ì›ë¬¸", res['link'])
            with b3:
                if st.button("â• ì¶”ê°€", key=f"add_{i}"):
                    summary = st.session_state.summaries.get(res['link'], "ìš”ì•½ë˜ì§€ ì•ŠìŒ")
                    item = f"ã…‡ {res['title']}_{res['press']}\n(âœ¨ AIìš”ì•½: {summary})\n{res['link']}\n\n"
                    if item not in st.session_state.scrap_list:
                        st.session_state.scrap_list.append(item)
                        st.toast("ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()