import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import pandas as pd

# --- [í¬ë¡¤ë§ ë¡œì§ í´ë˜ìŠ¤] ---
class NewsScraper:
    def fetch_news(self, start_datetime, end_datetime, keyword, photo_value):
        ds, de = start_datetime.strftime("%Y.%m.%d"), end_datetime.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_datetime.strftime('%Y%m%d')}to{end_datetime.strftime('%Y%m%d')}"
        
        all_results = []
        seen_links = set()
        scraper = cloudscraper.create_scraper()
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

        # ì›¹ ë²„ì „ì—ì„œëŠ” ì†ë„ë¥¼ ìœ„í•´ 3í˜ì´ì§€(30ê±´) ì •ë„ë§Œ ê¸ë„ë¡ ì„¤ì • (ì¡°ì ˆ ê°€ëŠ¥)
        for page in range(1, 4):
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query=\"{keyword}\"&sm=tab_pge&sort=1&photo={photo_value}&pd=3&ds={ds}&de={de}&nso={nso}&start={start_index}"
            
            try:
                response = scraper.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # SDS ë””ìì¸ ëŒ€ì‘ íŒŒì‹±
                sds_titles = soup.select('a[data-heatmap-target=".tit"]')
                if not sds_titles: break # ê²°ê³¼ ì—†ìœ¼ë©´ ì¢…ë£Œ

                for t_tag in sds_titles:
                    title = t_tag.get_text(strip=True)
                    link = t_tag.get('href')
                    if link in seen_links: continue
                    
                    # ì¹´ë“œ ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ì–¸ë¡ ì‚¬/ì‹œê°„ ì¶”ì¶œìš©)
                    card = t_tag.find_parent('div', class_=lambda c: c and ('api_subject_bx' in c or 'sds-comps' in c))
                    press = "ì•Œ ìˆ˜ ì—†ìŒ"
                    date_text = "ë‚ ì§œ ë¯¸ìƒ"
                    
                    if card:
                        press_el = card.select_one(".sds-comps-profile-info-title-text, .press_name")
                        if press_el: press = press_el.get_text(strip=True)
                        
                        subtexts = card.select(".sds-comps-profile-info-subtext, .info")
                        for sub in subtexts:
                            txt = sub.get_text(strip=True)
                            if 'ì „' in txt or ('.' in txt and txt[0].isdigit()):
                                date_text = txt
                                break

                    seen_links.add(link)
                    all_results.append({'title': title, 'link': link, 'press': press, 'time': date_text})
                time.sleep(0.3)
            except: break
        return all_results

# --- [ì›¹ UI ì‹œì‘] ---
st.set_page_config(page_title="ì„œìš¸êµí†µê³µì‚¬ ë‰´ìŠ¤ìŠ¤í¬ë©", layout="wide")

# ëª¨ë°”ì¼ ìµœì í™” ìŠ¤íƒ€ì¼ ì¶”ê°€
st.markdown("""
    <style>
    .main { background-color: #f8f9fa; }
    .stButton>button { width: 100%; border-radius: 10px; height: 3em; background-color: #007bff; color: white; }
    .news-card { background: white; padding: 15px; border-radius: 10px; border: 1px solid #ddd; margin-bottom: 10px; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸš‡ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìŠ¤í¬ë© (ëª¨ë°”ì¼)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìŠ¤í¬ë© ëª©ë¡ ì €ì¥ìš©)
if 'scrap_list' not in st.session_state:
    st.session_state.scrap_list = []

# ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ê²€ìƒ‰ ì„¤ì • (í„°ì¹˜í•´ì„œ ì—´ê¸°)", expanded=True):
    keyword = st.text_input("ê²€ìƒ‰ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("ì‹œì‘ì¼", datetime.date.today() - datetime.timedelta(days=1))
    with col2:
        end_date = st.date_input("ì¢…ë£Œì¼", datetime.date.today())

if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘"):
    scraper = NewsScraper()
    with st.spinner('ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ì½ì–´ì˜¤ëŠ” ì¤‘...'):
        results = scraper.fetch_news(start_date, end_date, keyword, 0)
        st.session_state.search_results = results

# ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
if 'search_results' in st.session_state:
    st.subheader(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ({len(st.session_state.search_results)}ê±´)")
    for i, res in enumerate(st.session_state.search_results):
        with st.container():
            st.markdown(f"""
            <div class="news-card">
                <strong>[{res['press']}]</strong> {res['title']}<br>
                <small style="color:gray;">{res['time']}</small>
            </div>
            """, unsafe_allow_html=True)
            col_a, col_b = st.columns([0.8, 0.2])
            with col_a:
                st.link_button("ê¸°ì‚¬ ì›ë¬¸ ë³´ê¸°", res['link'])
            with col_b:
                if st.button("â• ì¶”ê°€", key=f"add_{i}"):
                    item = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
                    if item not in st.session_state.scrap_list:
                        st.session_state.scrap_list.append(item)
                        st.toast("ëª©ë¡ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ìŠ¤í¬ë© ëª©ë¡ (í´ë¦½ë³´ë“œ ë³µì‚¬ìš©)
st.divider()
st.subheader("ğŸ“‹ ìµœì¢… ìŠ¤í¬ë© ëª©ë¡")
if st.session_state.scrap_list:
    final_text = "".join(st.session_state.scrap_list)
    st.text_area("ì•„ë˜ ë‚´ìš©ì„ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”", value=final_text, height=250)
    if st.button("ğŸ—‘ï¸ ëª©ë¡ ë¹„ìš°ê¸°"):
        st.session_state.scrap_list = []
        st.rerun()
else:
    st.info("ì¶”ê°€ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê¸°ì‚¬ë¥¼ ë‹´ìœ¼ì„¸ìš”.")