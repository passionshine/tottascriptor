import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import streamlit.components.v1 as components

# --- [1. ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜] ---
def get_target_date():
    today = datetime.date.today()
    if today.weekday() == 4: target = today + datetime.timedelta(days=3)
    elif today.weekday() == 5: target = today + datetime.timedelta(days=2)
    else: target = today + datetime.timedelta(days=1)

    holidays = [
        datetime.date(2025,1,1), datetime.date(2025,1,28), datetime.date(2025,1,29), datetime.date(2025,1,30),
        datetime.date(2025,3,1), datetime.date(2025,3,3), datetime.date(2025,5,5), datetime.date(2025,5,6),
        datetime.date(2025,6,6), datetime.date(2025,8,15), datetime.date(2025,10,3), datetime.date(2025,10,5),
        datetime.date(2025,10,6), datetime.date(2025,10,7), datetime.date(2025,10,8), datetime.date(2025,10,9), datetime.date(2025,12,25),
    ]
    while target in holidays or target.weekday() >= 5:
        target += datetime.timedelta(days=1)
    return target

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/'
        }

    def fetch_news(self, start_d, end_d, keyword, max_articles):
        ds, de = start_d.strftime("%Y.%m.%d"), end_d.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_d.strftime('%Y%m%d')}to{end_d.strftime('%Y%m%d')}"
        all_results, seen_links = [], set()
        query = f'"{keyword}"'
        max_pages = (max_articles // 10) + 1
        
        for page in range(max_pages):
            if len(all_results) >= max_articles: break
            start_val = (page * 10) + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&start={start_val}"
            try:
                res = self.scraper.get(url, headers=self.headers, timeout=10)
                soup = BeautifulSoup(res.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]')
                for t_tag in items:
                    if len(all_results) >= max_articles: break
                    title = t_tag.get('title') if t_tag.get('title') else t_tag.get_text(strip=True)
                    link = t_tag.get('href')
                    if link in seen_links: continue
                    seen_links.add(link)
                    
                    press_name = "ì•Œ ìˆ˜ ì—†ìŒ"
                    card = t_tag
                    for _ in range(5):
                        if card.parent:
                            card = card.parent
                            p_el = card.select_one(".sds-comps-profile-info-title-text, .press_name, .info.press")
                            if p_el: 
                                press_name = p_el.get_text(strip=True)
                                break
                    
                    all_results.append({'title': title, 'link': link, 'press': press_name})
                time.sleep(0.1)
            except: break
        return all_results

# --- [3. UI ë° ëª¨ë°”ì¼ ë°€ì°© ë ˆì´ì•„ì›ƒ CSS] ---
st.set_page_config(page_title="ë˜íƒ€ ìŠ¤í¬ë¦½í„°", layout="wide")

st.markdown("""
    <style>
    /* ì»¬ëŸ¼ ê°„ê²© ì™„ì „ ì œê±° ë° ê°€ë¡œ ë°°ì¹˜ ê°•ì œ */
    [data-testid="stHorizontalBlock"] { gap: 0px !important; }
    [data-testid="column"] { 
        flex-direction: row !important; 
        align-items: center !important; 
        padding: 0 1px !important; 
    }

    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼: í­ ì¢ê²Œ ìµœì í™” */
    .stButton > button, .stLinkButton > a {
        width: 100% !important; height: 38px !important;
        font-size: 11px !important; font-weight: 800 !important;
        padding: 0px !important; border-radius: 4px !important;
    }

    /* ë²„íŠ¼ ìƒ‰ìƒ */
    div[data-testid="column"]:nth-of-type(3) button { background-color: #D1E9FF !important; color: #004085 !important; }
    div[data-testid="column"]:nth-of-type(4) button { background-color: #E2F0D9 !important; color: #385723 !important; }

    /* ë‰´ìŠ¤ ì¹´ë“œ: ë„ˆë¹„ ëŒ€í­ í™•ì¥ */
    .news-card {
        background: white; padding: 10px; border-radius: 8px;
        border-left: 5px solid #007bff; box-shadow: 0 1px 2px rgba(0,0,0,0.1);
        width: 100%; margin-right: 5px;
    }
    .news-title { 
        font-size: 13px !important; font-weight: 700; color: #111; line-height: 1.3;
        display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden;
    }
    .news-meta { font-size: 9px !important; color: #666; margin-top: 2px; }
    </style>
    """, unsafe_allow_html=True)

for key in ['corp_list', 'rel_list', 'search_results']:
    if key not in st.session_state: st.session_state[key] = []

st.title("ğŸš‡ ë˜íƒ€ ìŠ¤í¬ë¦½í„°")

# ìŠ¤í¬ë© ê²°ê³¼ ì˜ì—­
t_date = get_target_date()
date_header = f"<{t_date.month}ì›” {t_date.day}ì¼ ì¡°ê°„ ìŠ¤í¬ë©>"
final_output = f"{date_header}\n\n[ê³µì‚¬ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.corp_list) + "\n[ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.rel_list)
st.text_area("ê²°ê³¼", value=final_output, height=150)

c_a, c_b = st.columns(2)
with c_a:
    if st.button("ğŸ“‹ ë³µì‚¬", use_container_width=True):
        st.toast("ë³µì‚¬ ì™„ë£Œ!")
        components.html(f"<script>navigator.clipboard.writeText(`{final_output}`);</script>", height=0)
with c_b:
    if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.corp_list, st.session_state.rel_list = [], []
        st.rerun()

st.divider()

# ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ê²€ìƒ‰ ì„¤ì •", expanded=True):
    keyword = st.text_input("ê²€ìƒ‰ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    max_a = st.slider("ê¸°ì‚¬ ìˆ˜", 10, 100, 30)
    if st.button("ğŸš€ ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.search_results = NewsScraper().fetch_news(datetime.date.today()-datetime.timedelta(days=1), datetime.date.today(), keyword, max_a)
        st.rerun()

# 3. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ (í† ìŠ¤íŠ¸ ë¡œì§ ê°œì„ )
if st.session_state.search_results:
    for i, res in enumerate(st.session_state.search_results):
        with st.container():
            # ë¹„ìœ¨ ì¡°ì •: ì¹´ë“œ 76%, ë²„íŠ¼ë“¤ ê° 8%ì”© ë°€ì°©
            col1, col2, col3, col4 = st.columns([0.76, 0.08, 0.08, 0.08])
            with col1:
                st.markdown(f'<div class="news-card"><div class="news-title">{res["title"]}</div><div class="news-meta">[{res["press"]}]</div></div>', unsafe_allow_html=True)
            with col2:
                st.link_button("ì›ë¬¸", res['link'])
            with col3:
                if st.button("ê³µì‚¬+", key=f"c_{i}"):
                    item = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
                    if item not in st.session_state.corp_list:
                        st.session_state.corp_list.append(item)
                        st.toast("ğŸ¢ ê³µì‚¬ ì¶”ê°€ ì™„ë£Œ!")
                        time.sleep(0.4)
                        st.rerun()
                    else:
                        st.toast("âš ï¸ ì´ë¯¸ ì¶”ê°€ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤.")
            with col4:
                if st.button("ìœ ê´€+", key=f"r_{i}"):
                    item = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
                    if item not in st.session_state.rel_list:
                        st.session_state.rel_list.append(item)
                        st.toast("ğŸš† ìœ ê´€ ì¶”ê°€ ì™„ë£Œ!")
                        time.sleep(0.4)
                        st.rerun()
                    else:
                        st.toast("âš ï¸ ì´ë¯¸ ì¶”ê°€ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤.")
        st.write("")