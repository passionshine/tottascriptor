import streamlit as st
import requests
from bs4 import BeautifulSoup
import datetime
import time
import re
import streamlit.components.v1 as components

# --- [1. ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜] ---
def get_target_date():
    today = datetime.date.today()
    if today.weekday() == 4: target = today + datetime.timedelta(days=3)
    elif today.weekday() == 5: target = today + datetime.timedelta(days=2)
    else: target = today + datetime.timedelta(days=1)

    holidays = [
        datetime.date(2025,1,1), datetime.date(2025,1,28), datetime.date(2025,1,29), datetime.date(2025,1,30),
        datetime.date(2025,3,1), datetime.date(2025,3,3), datetime.date(2025,5,5), datetime.date(2025,5,6),
        datetime.date(2025,6,6), datetime.date(2025,8,15), datetime.date(2025,10,3), datetime.date(2025,10,5),
        datetime.date(2025,10,6), datetime.date(2025,10,7), datetime.date(2025,10,8), datetime.date(2025,10,9), datetime.date(2025,12,25),
    ]
    while target in holidays or target.weekday() >= 5:
        target += datetime.timedelta(days=1)
    return target

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ (Pure HTML Parsing)] ---
class NewsScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }

    def fetch_news(self, start_d, end_d, keyword, max_articles):
        ds, de = start_d.strftime("%Y.%m.%d"), end_d.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_d.strftime('%Y%m%d')}to{end_d.strftime('%Y%m%d')}"
        all_results = []
        seen_titles = set()
        
        query = f'"{keyword}"'
        max_pages = (max_articles // 10) + 1
        
        # ë””ë²„ê¹… ë¡œê·¸ì°½
        log_container = st.container()
        status_text = st.empty()

        with log_container:
            st.info(f"ğŸš€ HTML ì§ì ‘ íŒŒì‹± ëª¨ë“œ ì‹œì‘: {keyword}")

        for page in range(max_pages):
            if len(all_results) >= max_articles: break
            
            start_val = (page * 10) + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&start={start_val}"
            
            status_text.text(f"â³ {page+1}í˜ì´ì§€ ê¸ì–´ì˜¤ëŠ” ì¤‘... (í˜„ì¬ {len(all_results)}ê°œ)")
            
            try:
                res = requests.get(url, headers=self.headers, timeout=10)
                if res.status_code != 200:
                    with log_container: st.error(f"âŒ ì ‘ì† ì‹¤íŒ¨: {res.status_code}")
                    break

                soup = BeautifulSoup(res.text, 'html.parser')

                # [í•µì‹¬] JSONì´ê³  ë­ê³  ë‹¤ ë²„ë¦¬ê³ , HTML êµ¬ì¡°(ul.list_news)ë¥¼ ë°”ë¡œ ì°¾ìŠµë‹ˆë‹¤.
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ê²°ê³¼ëŠ” ë³´í†µ <ul class="list_news"> ì•ˆì— <li class="bx">ë¡œ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
                news_items = soup.select("ul.list_news > li.bx")
                
                # ë§Œì•½ ìœ„ ì„ íƒìë¡œ ì•ˆ ì¡íˆë©´ ë‹¤ë¥¸ êµ¬ì¡°(div.news_wrap) ì‹œë„
                if not news_items:
                    news_items = soup.select("div.news_wrap")

                if not news_items:
                    with log_container: st.warning(f"âš ï¸ {page+1}í˜ì´ì§€: ë‰´ìŠ¤ ëª©ë¡ íƒœê·¸ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ë´‡ ì°¨ë‹¨)")
                    continue

                page_count = 0
                for item in news_items:
                    if len(all_results) >= max_articles: break
                    
                    # 1. ì œëª© ë° ì›ë³¸ ë§í¬ (a.news_tit)
                    title_tag = item.select_one("a.news_tit")
                    if not title_tag: continue
                    
                    raw_title = title_tag.get_text(strip=True)
                    original_link = title_tag['href']
                    
                    if raw_title in seen_titles: continue
                    seen_titles.add(raw_title)

                    # 2. ì–¸ë¡ ì‚¬ ì´ë¦„ (a.info.press)
                    press_tag = item.select_one("a.info.press")
                    press_name = press_tag.get_text(strip=True) if press_tag else "ì•Œ ìˆ˜ ì—†ìŒ"

                    # 3. [ì¤‘ìš”] ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ & ì§€ë©´ ì •ë³´ ì°¾ê¸°
                    # info_group ì•ˆì˜ a íƒœê·¸ì™€ span íƒœê·¸ë¥¼ ë’¤ì§‘ë‹ˆë‹¤.
                    is_naver = False
                    final_link = original_link # ê¸°ë³¸ê°’ì€ ì–¸ë¡ ì‚¬ ë§í¬
                    paper_info = ""

                    # (A) ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬: "ë„¤ì´ë²„ë‰´ìŠ¤" í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ ì°¾ê¸°
                    info_links = item.select("a.info")
                    for a_tag in info_links:
                        if "ë„¤ì´ë²„ë‰´ìŠ¤" in a_tag.get_text():
                            is_naver = True
                            final_link = a_tag['href']
                            break # ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                    
                    # (B) ì§€ë©´ ì •ë³´: "A1ë©´", "1ë©´" ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ span íƒœê·¸ ì°¾ê¸°
                    info_spans = item.select("span.info")
                    for span in info_spans:
                        txt = span.get_text(strip=True)
                        if re.search(r'[A-Za-z]*\d+ë©´', txt):
                            paper_info = f" ({txt})"
                    
                    # ì œëª© í•©ì¹˜ê¸°
                    full_title = f"{raw_title}{paper_info}"

                    all_results.append({
                        'title': full_title,
                        'link': final_link,
                        'press': press_name,
                        'is_naver': is_naver
                    })
                    page_count += 1
                
                # with log_container: st.write(f"âœ… {page+1}í˜ì´ì§€: {page_count}ê±´ ìˆ˜ì§‘")
                time.sleep(0.5)

            except Exception as e:
                with log_container: st.error(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
                continue
        
        status_text.empty()
        if not all_results:
            with log_container: st.error("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ê²€ìƒ‰ì–´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        return all_results

# --- [3. UI ì„¤ì •] ---
st.set_page_config(page_title="Totta Scraper (HTML Ver)", layout="wide")

st.markdown("""
    <style>
    [data-testid="stHorizontalBlock"] { gap: 4px !important; align-items: center !important; }
    div[data-testid="column"], div[data-testid="stColumn"] { padding: 0px !important; display: flex !important; justify-content: center !important; }
    .stButton > button { width: 100% !important; height: 38px !important; border-radius: 6px !important; }
    .stLinkButton > a { width: 100% !important; height: 38px !important; display: flex; align-items: center; justify-content: center; font-size: 11px !important; }
    
    .news-card { padding: 8px 12px; border-radius: 6px; border-left: 4px solid #007bff; box-shadow: 0 1px 1px rgba(0,0,0,0.05); display: flex; flex-direction: column; justify-content: center; height: 100%; }
    .bg-scraped { background: #eee !important; border-left: 4px solid #888 !important; opacity: 0.7; }
    .bg-white { background: white !important; }
    .news-title { font-size: 16px !important; font-weight: 600; color: #333; line-height: 1.2; margin-bottom: 2px; }
    .news-meta { font-size: 13px !important; color: #666; }
    .section-header { font-size: 18px; font-weight: 700; color: #333; margin-top: 20px; margin-bottom: 10px; border-bottom: 2px solid #007bff; display: inline-block; }
    
    div[data-testid="stHorizontalBlock"] > div:nth-child(3) button { background-color: #e3f2fd !important; color: #1565c0 !important; border: 1px solid #90caf9 !important; }
    div[data-testid="stHorizontalBlock"] > div:nth-child(4) button { background-color: #e8f5e9 !important; color: #2e7d32 !important; border: 1px solid #a5d6a7 !important; }
    </style>
    """, unsafe_allow_html=True)

# ì„¸ì…˜ ì´ˆê¸°í™”
for key in ['corp_list', 'rel_list', 'search_results']:
    if key not in st.session_state: st.session_state[key] = []

st.title("ğŸš‡ ë˜íƒ€ ìŠ¤í¬ë¦½í„° (HTML íŒŒì‹± ë²„ì „)")

# 1. ê²°ê³¼ ì˜ì—­
t_date = get_target_date()
date_header = f"<{t_date.month}ì›” {t_date.day}ì¼ ì¡°ê°„ ìŠ¤í¬ë©>"
final_output = f"{date_header}\n\n[ê³µì‚¬ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.corp_list) + "\n[ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.rel_list)

st.text_area("ğŸ“‹ ìŠ¤í¬ë© ê²°ê³¼", value=final_output, height=max(180, (final_output.count('\n') + 1) * 25))

c1, c2 = st.columns(2)
with c1:
    if st.button("ğŸ“‹ í…ìŠ¤íŠ¸ ë³µì‚¬", use_container_width=True):
        st.toast("ë³µì‚¬ ì™„ë£Œ!")
        components.html(f"<script>navigator.clipboard.writeText(`{final_output}`);</script>", height=0)
with c2:
    if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.corp_list, st.session_state.rel_list = [], []
        st.rerun()

# ê°œë³„ ê´€ë¦¬
with st.expander("ğŸ› ï¸ ìŠ¤í¬ë© í•­ëª© ê´€ë¦¬", expanded=False):
    st.write("**ğŸ¢ ê³µì‚¬ ë³´ë„**")
    for idx, item in enumerate(st.session_state.corp_list):
        ct, cd = st.columns([0.85, 0.15])
        with ct: st.caption(item.split('\n')[0])
        with cd: 
            if st.button("ì‚­ì œ", key=f"d_c_{idx}"): st.session_state.corp_list.pop(idx); st.rerun()
    st.write("**ğŸš† ìœ ê´€ê¸°ê´€ ë³´ë„**")
    for idx, item in enumerate(st.session_state.rel_list):
        ct, cd = st.columns([0.85, 0.15])
        with ct: st.caption(item.split('\n')[0])
        with cd:
            if st.button("ì‚­ì œ", key=f"d_r_{idx}"): st.session_state.rel_list.pop(idx); st.rerun()

st.divider()

# 2. ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì„¤ì •", expanded=True):
    keyword = st.text_input("ê²€ìƒ‰ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    d1, d2 = st.columns(2)
    with d1: start_d = st.date_input("ì‹œì‘ì¼", datetime.date.today() - datetime.timedelta(days=1))
    with d2: end_d = st.date_input("ì¢…ë£Œì¼", datetime.date.today())
    max_a = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 100, 30)
    
    if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.search_results = NewsScraper().fetch_news(start_d, end_d, keyword, max_a)

# 3. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
def display_list(title, items, key_prefix):
    st.markdown(f'<div class="section-header">{title} ({len(items)}ê±´)</div>', unsafe_allow_html=True)
    if not items:
        st.caption("ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for i, res in enumerate(items):
        item_txt = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
        is_scraped = (item_txt in st.session_state.corp_list) or (item_txt in st.session_state.rel_list)
        bg = "bg-scraped" if is_scraped else "bg-white"

        with st.container():
            c1, c2, c3, c4 = st.columns([0.73, 0.09, 0.09, 0.09])
            with c1:
                st.markdown(f'''<div class="news-card {bg}">
                    <div class="news-title">{res["title"]}</div>
                    <div class="news-meta">[{res["press"]}] {"(ìŠ¤í¬ë©ë¨)" if is_scraped else ""}</div>
                </div>''', unsafe_allow_html=True)
            with c2: st.link_button("ì›ë¬¸", res['link'])
            with c3:
                if st.button("ê³µì‚¬", key=f"c_{key_prefix}_{i}"):
                    if item_txt not in st.session_state.corp_list:
                        st.session_state.corp_list.append(item_txt)
                        st.toast("ğŸ¢ ì¶”ê°€ë¨"); time.sleep(0.1); st.rerun()
            with c4:
                if st.button("ìœ ê´€", key=f"r_{key_prefix}_{i}"):
                    if item_txt not in st.session_state.rel_list:
                        st.session_state.rel_list.append(item_txt)
                        st.toast("ğŸš† ì¶”ê°€ë¨"); time.sleep(0.1); st.rerun()
        st.markdown("<hr style='margin: 3px 0; border: none; border-top: 1px solid #f0f0f0;'>", unsafe_allow_html=True)

if st.session_state.search_results:
    naver_news = [x for x in st.session_state.search_results if x['is_naver']]
    other_news = [x for x in st.session_state.search_results if not x['is_naver']]
    
    display_list("ğŸŸ¢ ë„¤ì´ë²„ ë‰´ìŠ¤", naver_news, "n")
    st.write("")
    display_list("ğŸŒ ì–¸ë¡ ì‚¬ ìì²´ ê¸°ì‚¬", other_news, "o")