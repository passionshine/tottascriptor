import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import re
import streamlit.components.v1 as components

# --- [1. ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜ (2026ë…„ ê³µíœ´ì¼ ë°˜ì˜)] ---
def get_target_date():
    today = datetime.date.today()
    # ê¸ˆìš”ì¼ì´ë©´ ì›”ìš”ì¼(3ì¼ ë’¤), í† ìš”ì¼ì´ë©´ ì›”ìš”ì¼(2ì¼ ë’¤), ë‚˜ë¨¸ì§€ëŠ” ë‹¤ìŒë‚ 
    if today.weekday() == 4: target = today + datetime.timedelta(days=3)
    elif today.weekday() == 5: target = today + datetime.timedelta(days=2)
    else: target = today + datetime.timedelta(days=1)

    # 2026ë…„ ì£¼ìš” ê³µíœ´ì¼ (ëŒ€ì²´ê³µíœ´ì¼ í¬í•¨)
    holidays = [
        datetime.date(2026,1,1),   # ì‹ ì •
        datetime.date(2026,2,16), datetime.date(2026,2,17), datetime.date(2026,2,18), # ì„¤ë‚ 
        datetime.date(2026,3,1), datetime.date(2026,3,2), # ì‚¼ì¼ì ˆ ë° ëŒ€ì²´
        datetime.date(2026,5,5),   # ì–´ë¦°ì´ë‚ 
        datetime.date(2026,5,24), datetime.date(2026,5,25), # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚  ë° ëŒ€ì²´
        datetime.date(2026,6,6),   # í˜„ì¶©ì¼
        datetime.date(2026,8,15),  # ê´‘ë³µì ˆ
        datetime.date(2026,9,24), datetime.date(2026,9,25), datetime.date(2026,9,26), # ì¶”ì„
        datetime.date(2026,10,3),  # ê°œì²œì ˆ
        datetime.date(2026,10,9),  # í•œê¸€ë‚ 
        datetime.date(2026,12,25)  # ì„±íƒ„ì ˆ
    ]
    
    # ëª©í‘œì¼ì´ ê³µíœ´ì¼ì´ê±°ë‚˜ ì£¼ë§ì´ë©´ ë‹¤ìŒ í‰ì¼ë¡œ ì´ë™
    while target in holidays or target.weekday() >= 5:
        target += datetime.timedelta(days=1)
    return target

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë¦½í„°] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/'
        }

    def fetch_news(self, start_d, end_d, keyword, max_articles):
        ds, de = start_d.strftime("%Y.%m.%d"), end_d.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_d.strftime('%Y%m%d')}to{end_d.strftime('%Y%m%d')}"
        
        all_results = []
        seen_links = set()
        query = f'"{keyword}"'
        max_pages = (max_articles // 10) + 1
        
        status_text = st.empty()
        progress_bar = st.progress(0)

        status_text.text("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        for page in range(1, max_pages + 1):
            if len(all_results) >= max_articles: break
            
            progress_bar.progress(min(page / max_pages, 1.0))
            status_text.text(f"â³ {page}/{max_pages}í˜ì´ì§€ ë¶„ì„ ì¤‘... (í˜„ì¬ {len(all_results)}ê±´)")
            
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&qdt=1&start={start_index}"
            
            try:
                response = self.scraper.get(url, headers=self.headers, timeout=10)
                if response.status_code != 200: continue

                soup = BeautifulSoup(response.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]') or soup.select('a.news_tit')
                
                if not items: break

                for t_tag in items:
                    if len(all_results) >= max_articles: break

                    title = t_tag.get_text(strip=True)
                    original_link = t_tag.get('href')
                    
                    card = None
                    curr = t_tag
                    for _ in range(5):
                        if curr.parent:
                            curr = curr.parent
                            if curr.select_one(".sds-comps-profile") or curr.select_one(".news_info") or 'bx' in curr.get('class', []):
                                card = curr
                                break
                    
                    final_link = original_link
                    is_naver = "n.news.naver.com" in original_link
                    press_name = "ì•Œ ìˆ˜ ì—†ìŒ"
                    paper_info = ""
                    article_date = ""
                    is_paper = False

                    if card:
                        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ìš°ì„ 
                        naver_btn = card.select_one('a[href*="n.news.naver.com"]')
                        if naver_btn:
                            final_link = naver_btn.get('href')
                            is_naver = True
                        
                        # ì–¸ë¡ ì‚¬ëª…
                        press_el = card.select_one(".sds-comps-profile-info-title-text, .press_name, .info.press")
                        if press_el: press_name = press_el.get_text(strip=True)
                        
                        full_text = card.get_text(separator=" ", strip=True)
                        
                        # ë‚ ì§œ íŒŒì‹± (ìƒëŒ€ì‹œê°„ + ì ˆëŒ€ë‚ ì§œ ëª¨ë‘ ëŒ€ì‘)
                        date_match = re.search(r'(\d+\s?(?:ë¶„|ì‹œê°„|ì¼|ì£¼|ì´ˆ)\s?ì „|ë°©ê¸ˆ\s?ì „)', full_text)
                        abs_date_match = re.search(r'(\d{4}[\.\-]\d{2}[\.\-]\d{2})', full_text)

                        if date_match:
                            article_date = date_match.group(1)
                        elif abs_date_match:
                            article_date = abs_date_match.group(1).rstrip('.')
                        
                        # ì§€ë©´ ì •ë³´
                        if re.search(r'([A-Za-z]*\d+ë©´)', full_text):
                            paper_info = " (ì§€ë©´)"
                            is_paper = True

                    if final_link in seen_links: continue
                    seen_links.add(final_link)
                    
                    all_results.append({
                        'title': f"{title}{paper_info}",
                        'link': final_link,
                        'press': press_name,
                        'is_naver': is_naver,
                        'is_paper': is_paper,
                        'date': article_date
                    })
                time.sleep(0.3)
            except: continue
        
        progress_bar.empty()
        status_text.empty()
        return all_results

# --- [3. UI ì„¤ì • ë° CSS] ---
st.set_page_config(page_title="Totta Scriptor for web", layout="wide")

st.markdown("""
    <style>
    /* ë‰´ìŠ¤ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .news-card { 
        padding: 12px 16px; border-radius: 8px; border-left: 5px solid #007bff; 
        box-shadow: 0 2px 4px rgba(0,0,0,0.08); background: white; margin-bottom: 5px;
    }
    .bg-scraped { background: #f8f9fa !important; border-left: 5px solid #adb5bd !important; opacity: 0.7; }
    .news-title { font-size: 17px !important; font-weight: 700; color: #222; margin-bottom: 5px; line-height: 1.4; }
    .news-meta { font-size: 15px !important; color: #666; }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ í†µì¼ (ì¼ë°˜ ë²„íŠ¼ + ë§í¬ ë²„íŠ¼) */
    .stButton > button, .stLinkButton > a { 
        width: 100% !important; 
        height: 38px !important; 
        font-size: 8px !important; 
        font-weight: 600 !important;
        padding: 0 !important;
        display: flex; align-items: center; justify-content: center;
        border-radius: 4px !important;
    }
    /* ë²„íŠ¼ ë‚´ë¶€ ì»¨í…Œì´ë„ˆì˜ íŒ¨ë”© ì œê±° */
    div[data-testid="stVerticalBlockBorderWrapper"] { padding: 5px !important; }
    
    .section-header { font-size: 17px; font-weight: 700; color: #333; margin: 25px 0 10px 0; border-bottom: 2px solid #007bff; display: inline-block; }
    </style>
    """, unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
for key in ['corp_list', 'rel_list', 'search_results']:
    if key not in st.session_state: st.session_state[key] = []

st.title("ğŸš‡ Totta Scriptor for web")

# 1. ê²°ê³¼ ì¶œë ¥ ì˜ì—­
t_date = get_target_date()
weekdays = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
w_str = weekdays[t_date.weekday()]

# ì¶œë ¥ ì˜ˆì‹œ: <12ì›” 23ì¼(í™”) ì¡°ê°„ ìŠ¤í¬ë©>
date_header = f"<{t_date.month}ì›” {t_date.day}ì¼({w_str}) ì¡°ê°„ ìŠ¤í¬ë©>"




final_output = f"{date_header}\n\n[ê³µì‚¬ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.corp_list) + "\n[ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.rel_list)
text_height = max(150, (final_output.count('\n') + 1) * 22)
st.text_area("ğŸ“‹ ìµœì¢… ìŠ¤í¬ë© í…ìŠ¤íŠ¸", value=final_output, height=text_height)

# ë³µì‚¬ ë²„íŠ¼ (JavaScript)
if final_output.strip() != date_header.strip():
    components.html(f"""
        <button onclick="copy()" style="width:100%; height:40px; background:#007bff; color:white; border:none; border-radius:5px; cursor:pointer; font-weight:bold;">ğŸ“‹ í…ìŠ¤íŠ¸ ë³µì‚¬í•˜ê¸°</button>
        <textarea id="t" style="position:absolute;top:-9999px">{final_output}</textarea>
        <script>function copy(){{var t=document.getElementById("t");t.select();document.execCommand("copy");alert("âœ… ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!");}}</script>
    """, height=50)

if st.button("ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”"):
    st.session_state.corp_list, st.session_state.rel_list = [], []
    st.rerun()

st.divider()

# 2. ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì„¤ì •", expanded=True):
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1: kw = st.text_input("ê²€ìƒ‰ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    with col2: sd = st.date_input("ì‹œì‘", datetime.date.today() - datetime.timedelta(days=1))
    with col3: ed = st.date_input("ì¢…ë£Œ", datetime.date.today())
    mx = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 100, 30)
    if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.search_results = NewsScraper().fetch_news(sd, ed, kw, mx)

# 3. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜ (ì¤‘ë³µ ì²´í¬ ë©”ì‹œì§€ ì¶”ê°€ + ë‚ ì§œ ì œê±°)
def display_list(title, items, key_p):
    st.markdown(f'<div class="section-header">{title} ({len(items)}ê±´)</div>', unsafe_allow_html=True)
    for i, res in enumerate(items):
        d_val = res.get('date', '')
        # í™”ë©´ í‘œì‹œìš© ë‚ ì§œ í¬ë§·
        d_str_display = f"[{d_val}] " if d_val else ""
        
        # [ìˆ˜ì •] ìŠ¤í¬ë© ê²°ê³¼ í…ìŠ¤íŠ¸ì—ëŠ” ë‚ ì§œë¥¼ ì œì™¸í•¨
        item_txt = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
        
        is_scraped = (item_txt in st.session_state.corp_list) or (item_txt in st.session_state.rel_list)
        bg = "bg-scraped" if is_scraped else ""

        col_m, col_b = st.columns([0.65, 0.35])
        
        with col_m:
            st.markdown(f"""<div class="news-card {bg}">
                <div class="news-title">{res['title']}</div>
                <div class="news-meta"><span style="color:#007bff;font-weight:bold;">{d_val}</span> | {res['press']}</div>
            </div>""", unsafe_allow_html=True)
        
        with col_b:
            with st.container(border=True):
                b1, b2, b3 = st.columns(3, gap="small")
                with b1: 
                    st.link_button("ì›ë¬¸ë³´ê¸°", res['link'], use_container_width=True)
                with b2:
                    if st.button("ê³µì‚¬ ê¸°ì‚¬", key=f"c_{key_p}_{i}", use_container_width=True):
                        if item_txt not in st.session_state.corp_list:
                            st.session_state.corp_list.append(item_txt)
                            st.toast("ê³µì‚¬ ê´€ë ¨ ë³´ë„ë¡œ ìŠ¤í¬ë©ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="âœ…")
                            time.sleep(1.0)
                            st.rerun()
                        else:
                            st.toast("âš ï¸ ì´ë¯¸ ìŠ¤í¬ë©ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤.", icon="â—")
                with b3:
                    if st.button("ê¸°íƒ€ ê¸°ì‚¬", key=f"r_{key_p}_{i}", use_container_width=True):
                        if item_txt not in st.session_state.rel_list:
                            st.session_state.rel_list.append(item_txt)
                            st.toast("ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„ë¡œ ìŠ¤í¬ë©ë˜ì—ˆìŠµë‹ˆë‹¤!", icon="âœ…")
                            time.sleep(1.0)
                            st.rerun()
                        else:
                            st.toast("âš ï¸ ì´ë¯¸ ìŠ¤í¬ë©ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤.", icon="â—")

# ë¶„ë¥˜ í›„ ì¶œë ¥
if st.session_state.search_results:
    res = st.session_state.search_results
    p_news = [x for x in res if x['is_paper']]
    n_news = [x for x in res if x['is_naver'] and not x['is_paper']]
    o_news = [x for x in res if not x['is_naver'] and not x['is_paper']]
    
    if p_news: display_list("ğŸ“° ì§€ë©´ ë³´ë„", p_news, "p")
    if n_news: display_list("ğŸŸ¢ ë„¤ì´ë²„ ë‰´ìŠ¤", n_news, "n")
    if o_news: display_list("ğŸŒ ì–¸ë¡ ì‚¬ ìì²´ ë‰´ìŠ¤", o_news, "o")







