import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import google.generativeai as genai

# --- [AI ì„¤ì •] ---
# Streamlitì˜ Secrets ê¸°ëŠ¥ì„ í†µí•´ ë³´ì•ˆìƒ ì•ˆì „í•˜ê²Œ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì§ì ‘ ë„£ìœ¼ì‹œë ¤ë©´ "ë³¸ì¸ì˜_API_í‚¤"ë¥¼ ì…ë ¥í•˜ì„¸ìš”)
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')
    AI_ENABLED = True
except:
    AI_ENABLED = False

# --- [ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤] ---
class NewsScraper:
    def summarize_text(self, title):
        if not AI_ENABLED:
            return "AI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìš”ì•½ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        try:
            # ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ AIì—ê²Œ í•œ ì¤„ ìš”ì•½ ìš”ì²­
            prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì„ ë¶„ì„í•´ì„œ 30ì ì´ë‚´ì˜ ì•„ì£¼ ì§§ì€ ìš”ì•½ë¬¸ í•œ ì¤„ë§Œ ë§Œë“¤ì–´ì¤˜: {title}"
            response = model.generate_content(prompt)
            return response.text.strip()
        except:
            return "ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def fetch_news(self, start_datetime, end_datetime, keyword, photo_value):
        ds, de = start_datetime.strftime("%Y.%m.%d"), end_datetime.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_datetime.strftime('%Y%m%d')}to{end_datetime.strftime('%Y%m%d')}"
        all_results, seen_links = [], set()
        scraper = cloudscraper.create_scraper()
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36', 'Referer': 'https://www.naver.com/'}

        query = f'"{keyword}"'
        for page in range(1, 4):
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo={photo_value}&pd=3&ds={ds}&de={de}&nso={nso}&start={start_index}"
            try:
                response = scraper.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]')
                if not items: break

                for t_tag in items:
                    title = t_tag.get_text(strip=True)
                    link = t_tag.get('href')
                    if link in seen_links: continue
                    
                    card = None
                    curr = t_tag
                    for _ in range(5):
                        if curr.parent:
                            curr = curr.parent
                            if curr.select_one(".sds-comps-profile") or curr.select_one(".news_info"):
                                card = curr; break
                    
                    final_link, is_naver, press_name, date_text = link, "n.news.naver.com" in link, "ì•Œ ìˆ˜ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"
                    if card:
                        naver_btn = card.select_one('a[href*="n.news.naver.com"]')
                        if naver_btn: final_link = naver_btn.get('href'); is_naver = True
                        press_el = card.select_one(".sds-comps-profile-info-title-text, .press_name, .info.press")
                        if press_el: press_name = press_el.get_text(strip=True)
                        subtext_area = card.select_one(".sds-comps-profile-info-subtexts, .news_info")
                        if subtext_area:
                            for txt in subtext_area.stripped_strings:
                                if ('ì „' in txt and len(txt) < 15) or ('.' in txt and len(txt) < 15 and txt[0].isdigit()):
                                    date_text = txt; break

                    # AI ìš”ì•½ ìƒì„±
                    summary = self.summarize_text(title)

                    seen_links.add(final_link)
                    all_results.append({'title': title, 'link': final_link, 'press': press_name, 'time': date_text, 'is_naver': is_naver, 'summary': summary})
                time.sleep(0.3)
            except: break
        return all_results

# --- [Streamlit ì›¹ UI] ---
st.set_page_config(page_title="ì„œìš¸êµí†µê³µì‚¬ AI ìŠ¤í¬ë©", layout="wide")

st.markdown("""
    <style>
    .stButton > button, .stLinkButton > a {
        display: inline-flex !important; align-items: center !important; justify-content: center !important;
        width: 100% !important; height: 40px !important; background-color: #ffffff !important;
        color: #31333F !important; border: 1px solid #d1d5db !important; border-radius: 8px !important;
        text-decoration: none !important; font-size: 14px !important; font-weight: 600 !important; margin: 0 !important;
    }
    .stButton > button:hover, .stLinkButton > a:hover { border-color: #007bff !important; color: #007bff !important; background-color: #f0f7ff !important; }
    .news-card { background: white; padding: 15px; border-radius: 12px; border-left: 6px solid #007bff; margin-bottom: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.05); }
    .ai-summary { background-color: #f3f0ff; color: #553c9a; padding: 10px; border-radius: 8px; font-size: 13px; margin-top: 10px; border-left: 4px solid #9f7aea; }
    </style>
    """, unsafe_allow_html=True)

if 'scrap_list' not in st.session_state: st.session_state.scrap_list = []
if 'search_results' not in st.session_state: st.session_state.search_results = []

st.title("ğŸš‡ AI ì´ìŠˆ ìŠ¤í¬ë˜í¼")

# 1. ìƒë‹¨: ê°€ë³€í˜• ìŠ¤í¬ë© ëª©ë¡
st.subheader("ğŸ“‹ ì‹¤ì‹œê°„ ìŠ¤í¬ë© ëª©ë¡")
if st.session_state.scrap_list:
    final_text = "".join(st.session_state.scrap_list)
    dynamic_height = min(max(150, len(st.session_state.scrap_list) * 55), 450)
    st.text_area("ì „ì²´ ì„ íƒ í›„ ë³µì‚¬í•˜ì„¸ìš”", value=final_text, height=dynamic_height)
    if st.button("ğŸ—‘ï¸ ëª©ë¡ ë¹„ìš°ê¸°"):
        st.session_state.scrap_list = []
        st.rerun()
else:
    st.info("â• ì¶”ê°€ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ AI ìš”ì•½ê³¼ í•¨ê»˜ ì—¬ê¸°ì— ì €ì¥ë©ë‹ˆë‹¤.")

st.divider()

# 2. ì¤‘ê°„: ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •", expanded=True):
    keyword = st.text_input("í•„ìˆ˜ í¬í•¨ ë‹¨ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    c1, c2 = st.columns(2)
    with c1: start_date = st.date_input("ì‹œì‘", datetime.date.today() - datetime.timedelta(days=1))
    with c2: end_date = st.date_input("ì¢…ë£Œ", datetime.date.today())
    filter_choice = st.radio("ê²€ìƒ‰ ë²”ìœ„", ["ë„¤ì´ë²„ ê¸°ì‚¬", "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬", "ëª¨ë‘ ë³´ê¸°"], index=0, horizontal=True)

if st.button("ğŸš€ AI ë¶„ì„ ë° ê²€ìƒ‰ ì‹œì‘", type="primary"):
    scraper = NewsScraper()
    with st.spinner('AIê°€ ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì”© ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
        results = scraper.fetch_news(start_date, end_date, keyword, 0)
        st.session_state.search_results = results

# 3. í•˜ë‹¨: ê²€ìƒ‰ ê²°ê³¼
if st.session_state.search_results:
    if filter_choice == "ë„¤ì´ë²„ ê¸°ì‚¬": display_results = [r for r in st.session_state.search_results if r['is_naver']]
    elif filter_choice == "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬": display_results = [r for r in st.session_state.search_results if not r['is_naver']]
    else: display_results = st.session_state.search_results

    st.subheader(f"âœ… ë¶„ì„ ê²°ê³¼: {len(display_results)}ê±´")
    for i, res in enumerate(display_results):
        with st.container():
            st.markdown(f"""
            <div class="news-card">
                <strong>[{res['press']}]</strong> {res['title']}<br>
                <small style="color:gray;">{res['time']} {'(ë„¤ì´ë²„ë‰´ìŠ¤)' if res['is_naver'] else ''}</small>
                <div class="ai-summary">âœ¨ <b>AI í•œì¤„ìš”ì•½:</b> {res['summary']}</div>
            </div>
            """, unsafe_allow_html=True)
            
            b1, b2 = st.columns(2)
            with b1: st.link_button("ğŸ”— ì›ë¬¸ë³´ê¸°", res['link'])
            with b2:
                if st.button("â• ëª©ë¡ ì¶”ê°€", key=f"add_{i}"):
                    # ìš”ì•½ë³¸ì„ í¬í•¨í•˜ì—¬ ìŠ¤í¬ë© í…ìŠ¤íŠ¸ êµ¬ì„±
                    item = f"ã…‡ {res['title']}_{res['press']}\n(ìš”ì•½: {res['summary']})\n{res['link']}\n\n"
                    if item not in st.session_state.scrap_list:
                        st.session_state.scrap_list.append(item)
                        st.toast("ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()