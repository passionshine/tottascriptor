import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import google.generativeai as genai

# --- [1. AI ì„¤ì •] ---
# Streamlit Cloudì˜ Settings > Secretsì— GOOGLE_API_KEYë¥¼ ë“±ë¡í•˜ì„¸ìš”.
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    HAS_AI = True
except:
    HAS_AI = False

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ í´ë˜ìŠ¤] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36', 'Referer': 'https://www.naver.com/'}

    # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜ (ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš©)
    def get_article_body(self, url):
        if "n.news.naver.com" not in url:
            return "" # ì™¸ë¶€ ì‚¬ì´íŠ¸ëŠ” êµ¬ì¡°ê°€ ë‹¤ì–‘í•˜ì—¬ ì¼ë‹¨ ìƒëµ
        try:
            res = self.scraper.get(url, headers=self.headers, timeout=5)
            soup = BeautifulSoup(res.content, 'html.parser')
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì„ íƒì
            content = soup.select_one("#newsct_article") or soup.select_one("#articleBodyContents")
            return content.get_text(strip=True)[:2000] if content else ""
        except:
            return ""

    # AI ìš”ì•½ ì‹¤í–‰ í•¨ìˆ˜
    def summarize_with_ai(self, title, body, keyword):
        if not HAS_AI:
            return f"'{keyword}' ê´€ë ¨ ì£¼ìš” ë³´ë„ ë‚´ìš©ì…ë‹ˆë‹¤. (API í‚¤ ë¯¸ì„¤ì •)"
        
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            if not body:
                prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì„ ë³´ê³  í•µì‹¬ì„ í•œ ì¤„ë¡œ ìš”ì•½í•´ì¤˜: {title}"
            else:
                prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì½ê³  '{keyword}' ì—…ë¬´ì™€ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ 1ì¤„ë¡œ ìš”ì•½í•´ì¤˜. \në³¸ë¬¸: {body}"
            
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return "ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def fetch_news(self, start_datetime, end_datetime, keyword):
        ds, de = start_datetime.strftime("%Y.%m.%d"), end_datetime.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_datetime.strftime('%Y%m%d')}to{end_datetime.strftime('%Y%m%d')}"
        all_results = []
        seen_links = set()
        
        query = f'"{keyword}"'
        for page in range(1, 4):
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&start={start_index}"
            try:
                response = self.scraper.get(url, headers=self.headers, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]')
                if not items: break

                for t_tag in items:
                    title = t_tag.get_text(strip=True)
                    link = t_tag.get('href')
                    if link in seen_links: continue
                    
                    # ì–¸ë¡ ì‚¬ ë° ì‹œê°„ ì •ë³´ íŒŒì‹± (ìƒëµ - ì´ì „ ë²„ì „ê³¼ ë™ì¼)
                    is_naver = "n.news.naver.com" in link
                    press_name = "ì–¸ë¡ ì‚¬"
                    date_text = "ì‹œê°„"
                    
                    # ì‹¤ì œ íŒŒì‹± ë¡œì§ (ì¹´ë“œ íƒìƒ‰)
                    card = t_tag.find_parent('div', class_=lambda c: c and ('api_subject_bx' in c or 'sds-comps' in c))
                    if card:
                        p_el = card.select_one(".sds-comps-profile-info-title-text, .press_name")
                        if p_el: press_name = p_el.get_text(strip=True)
                        t_el = card.select_one(".sds-comps-profile-info-subtexts")
                        if t_el: date_text = t_el.get_text(strip=True)

                    seen_links.add(link)
                    all_results.append({'title': title, 'link': link, 'press': press_name, 'time': date_text, 'is_naver': is_naver})
                time.sleep(0.3)
            except: break
        return all_results

# --- [3. Streamlit UI] ---
st.set_page_config(page_title="ì„œìš¸êµí†µê³µì‚¬ ìŠ¤í¬ë©", layout="wide")

st.markdown("""
    <style>
    .stButton > button, .stLinkButton > a {
        display: inline-flex !important; align-items: center !important; justify-content: center !important;
        width: 100% !important; height: 40px !important; background-color: #ffffff !important;
        color: #31333F !important; border: 1px solid #d1d5db !important; border-radius: 8px !important;
        font-size: 14px !important; font-weight: 600 !important;
    }
    .news-card {
        background: white; padding: 15px; border-radius: 12px; border-left: 6px solid #007bff;
        margin-bottom: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    .ai-summary-box {
        background-color: #f3f0ff; color: #553c9a; padding: 10px; border-radius: 8px;
        font-size: 13px; margin-top: 10px; border-left: 4px solid #9f7aea;
    }
    </style>
    """, unsafe_allow_html=True)

if 'scrap_list' not in st.session_state: st.session_state.scrap_list = []
if 'search_results' not in st.session_state: st.session_state.search_results = []

st.title("ğŸš‡ ë‰´ìŠ¤ ìŠ¤í¬ë© (ë³¸ë¬¸ ê¸°ë°˜ AI ìš”ì•½)")

# [ìµœìƒë‹¨] ê°€ë³€í˜• ìŠ¤í¬ë© ëª©ë¡
st.subheader("ğŸ“‹ ì‹¤ì‹œê°„ ìŠ¤í¬ë© ëª©ë¡")
if st.session_state.scrap_list:
    final_text = "".join(st.session_state.scrap_list)
    dynamic_height = min(max(150, len(st.session_state.scrap_list) * 55), 450)
    st.text_area("ë‚´ìš© ë³µì‚¬", value=final_text, height=dynamic_height)
    if st.button("ğŸ—‘ï¸ ì „ì²´ ë¹„ìš°ê¸°"):
        st.session_state.scrap_list = []
        st.rerun()
else:
    st.info("ê²€ìƒ‰ í›„ 'â• ì¶”ê°€'ë¥¼ ëˆ„ë¥´ë©´ AIê°€ ë³¸ë¬¸ì„ ìš”ì•½í•˜ì—¬ ëª©ë¡ì— ë‹´ìŠµë‹ˆë‹¤.")

st.divider()

# [ì¤‘ê°„] ê²€ìƒ‰ ì¡°ê±´
with st.expander("ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •", expanded=True):
    keyword_input = st.text_input("í•„ìˆ˜ ë‹¨ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    c1, c2 = st.columns(2)
    with c1: start_d = st.date_input("ì‹œì‘", datetime.date.today() - datetime.timedelta(days=1))
    with c2: end_d = st.date_input("ì¢…ë£Œ", datetime.date.today())
    filter_opt = st.radio("í•„í„°", ["ë„¤ì´ë²„ ê¸°ì‚¬", "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬", "ëª¨ë‘ ë³´ê¸°"], horizontal=True)

if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary"):
    sc = NewsScraper()
    with st.spinner('ê²€ìƒ‰ ì¤‘...'):
        st.session_state.search_results = sc.fetch_news(start_d, end_d, keyword_input)

# [í•˜ë‹¨] ê²°ê³¼ ì¶œë ¥
if st.session_state.search_results:
    res_list = st.session_state.search_results
    if filter_opt == "ë„¤ì´ë²„ ê¸°ì‚¬": res_list = [r for r in res_list if r['is_naver']]
    elif filter_opt == "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬": res_list = [r for r in res_list if not r['is_naver']]

    st.subheader(f"âœ… ê²°ê³¼: {len(res_list)}ê±´")
    for i, res in enumerate(res_list):
        with st.container():
            st.markdown(f"""
            <div class="news-card">
                <strong>[{res['press']}]</strong> {res['title']}<br>
                <small style="color:gray;">{res['time']} {'(ì¸ë§í¬)' if res['is_naver'] else ''}</small>
            </div>
            """, unsafe_allow_html=True)
            
            b1, b2 = st.columns(2)
            with b1: st.link_button("ğŸ”— ì›ë¬¸ë³´ê¸°", res['link'])
            with b2:
                if st.button("â• ì¶”ê°€ & ìš”ì•½", key=f"add_{i}"):
                    sc = NewsScraper()
                    with st.spinner('âœ¨ AIê°€ ë³¸ë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
                        # 1. ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸°
                        body_content = sc.get_article_body(res['link'])
                        # 2. AI ìš”ì•½ ì‹¤í–‰
                        ai_summary = sc.summarize_with_ai(res['title'], body_content, keyword_input)
                        # 3. ëª©ë¡ì— ì¶”ê°€
                        item = f"ã…‡ {res['title']}_{res['press']}\n(âœ¨ AIìš”ì•½: {ai_summary})\n{res['link']}\n\n"
                        if item not in st.session_state.scrap_list:
                            st.session_state.scrap_list.append(item)
                            st.toast("ìš”ì•½ê³¼ í•¨ê»˜ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()