import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import re
import streamlit.components.v1 as components

# --- [1. ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜ (2026ë…„ ê³µíœ´ì¼ ë°˜ì˜)] ---
def get_target_date():
    today = datetime.date.today()
    if today.weekday() == 4: target = today + datetime.timedelta(days=3)
    elif today.weekday() == 5: target = today + datetime.timedelta(days=2)
    else: target = today + datetime.timedelta(days=1)

    # 2026ë…„ ì£¼ìš” ê³µíœ´ì¼ (ëŒ€ì²´ê³µíœ´ì¼ í¬í•¨)
    holidays = [
        datetime.date(2026,1,1),  # ì‹ ì •
        datetime.date(2026,2,16), datetime.date(2026,2,17), datetime.date(2026,2,18), # ì„¤ë‚ 
        datetime.date(2026,3,1), datetime.date(2026,3,2), # ì‚¼ì¼ì ˆ ë° ëŒ€ì²´
        datetime.date(2026,5,5),  # ì–´ë¦°ì´ë‚ 
        datetime.date(2026,5,24), datetime.date(2026,5,25), # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚  ë° ëŒ€ì²´
        datetime.date(2026,6,6),  # í˜„ì¶©ì¼
        datetime.date(2026,8,15), # ê´‘ë³µì ˆ
        datetime.date(2026,9,24), datetime.date(2026,9,25), datetime.date(2026,9,26), # ì¶”ì„
        datetime.date(2026,10,3), # ê°œì²œì ˆ
        datetime.date(2026,10,9), # í•œê¸€ë‚ 
        datetime.date(2026,12,25) # ì„±íƒ„ì ˆ
    ]
    
    while target in holidays or target.weekday() >= 5:
        target += datetime.timedelta(days=1)
    return target

# --- [2. ë‚ ì§œ íŒŒì‹± í—¬í¼ í•¨ìˆ˜] ---
def parse_date_text(text):
    """
    '14ë¶„ ì „', '1ì‹œê°„ ì „', '1ì¼ ì „', '2025.12.23.' ë“±ì˜ í…ìŠ¤íŠ¸ë¥¼ 
    datetime.date ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    today = datetime.date.today()
    text = text.strip()
    
    # 1. ìƒëŒ€ ë‚ ì§œ ì²˜ë¦¬ (ë¶„/ì‹œê°„ ì „ -> ì˜¤ëŠ˜, ì¼ ì „ -> ê³„ì‚°)
    if "ì „" in text:
        if "ë¶„" in text or "ì‹œê°„" in text or "ë°©ê¸ˆ" in text:
            return today
        match = re.search(r'(\d+)ì¼', text)
        if match:
            days_ago = int(match.group(1))
            return today - datetime.timedelta(days=days_ago)
        return today # ê·¸ ì™¸ 'ì–´ì œ' ë“±ì€ ì˜¤ëŠ˜ë¡œ ê°„ì£¼í•˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬ ê°€ëŠ¥

    # 2. ì ˆëŒ€ ë‚ ì§œ ì²˜ë¦¬ (YYYY.MM.DD.)
    match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', text)
    if match:
        try:
            return datetime.date(int(match.group(1)), int(match.group(2)), int(match.group(3)))
        except:
            return None
    return None

# --- [3. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ (ê°œì„ ë¨)] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://search.naver.com/'
        }

    def fetch_news(self, start_d, end_d, keyword, max_articles):
        # ë‚ ì§œ í¬ë§· (YYYYMMDD)
        ds_param = start_d.strftime("%Y%m%d")
        de_param = end_d.strftime("%Y%m%d")
        
        # ê²€ìƒ‰ ì˜µì…˜: ìµœì‹ ìˆœ(so:dd), ê¸°ê°„(from~to)
        # ë„¤ì´ë²„ê°€ ì •í™•íˆ í•„í„°ë§í•´ì£¼ì§€ ì•Šì„ ë•Œë¥¼ ëŒ€ë¹„í•´ ë¡œì§ì—ì„œë„ ê²€ì‚¬í•¨
        nso = f"so:dd,p:from{ds_param}to{de_param},a:all"
        
        all_results = []
        seen_links = set()
        
        query = f'"{keyword}"'
        max_pages = (max_articles // 10) + 5 # ë„‰ë„‰í•˜ê²Œ í˜ì´ì§€ ìˆœíšŒ
        
        status_text = st.empty()
        progress_bar = st.progress(0)
        log_container = st.container()

        status_text.text("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        stop_crawling = False

        for page in range(1, max_pages + 1):
            if len(all_results) >= max_articles or stop_crawling: 
                break
            
            # ì§„í–‰ë¥  í‘œì‹œ
            current_count = len(all_results)
            progress = min(current_count / max_articles, 1.0)
            progress_bar.progress(progress)
            status_text.text(f"â³ {page}í˜ì´ì§€ ë¶„ì„ ì¤‘... (ìˆ˜ì§‘: {current_count}/{max_articles}ê±´)")
            
            start_index = (page - 1) * 10 + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={start_d.strftime('%Y.%m.%d')}&de={end_d.strftime('%Y.%m.%d')}&nso={nso}&start={start_index}"
            
            try:
                response = self.scraper.get(url, headers=self.headers, timeout=10)
                if response.status_code != 200:
                    time.sleep(1)
                    continue

                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë‰´ìŠ¤ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ (.news_wrap í´ë˜ìŠ¤ê°€ ê°€ì¥ ì •í™•í•¨)
                items = soup.select('.news_wrap')
                
                if not items:
                    with log_container: st.warning(f"âš ï¸ {page}í˜ì´ì§€: ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break

                for card in items:
                    if len(all_results) >= max_articles: break

                    # -- 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ --
                    title_tag = card.select_one('a.news_tit')
                    if not title_tag: continue
                    
                    title = title_tag.get_text(strip=True)
                    original_link = title_tag.get('href')
                    
                    # -- 2. ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ì–¸ë¡ ì‚¬, ë‚ ì§œ, ì§€ë©´) --
                    press_name = ""
                    date_text = ""
                    is_paper = False
                    paper_info = ""
                    
                    # .info_group ë‚´ë¶€ì˜ ìš”ì†Œë“¤ì„ ìˆœíšŒí•˜ë©° í™•ì¸
                    info_group = card.select_one('.info_group')
                    if info_group:
                        # ì–¸ë¡ ì‚¬
                        press_el = info_group.select_one('.press')
                        if press_el: 
                            press_name = press_el.get_text(strip=True)
                        
                        # ë‚˜ë¨¸ì§€ ì •ë³´ë“¤ (ë‚ ì§œ, ë„¤ì´ë²„ë‰´ìŠ¤ ë§í¬, ì§€ë©´ì •ë³´ ë“±)
                        infos = info_group.select('.info')
                        for info in infos:
                            txt = info.get_text(strip=True)
                            if "ë©´" in txt and "ì „" not in txt: # ì§€ë©´ ì •ë³´ (A10ë©´ ë“±)
                                is_paper = True
                                paper_info = " (ì§€ë©´)"
                            elif re.search(r'\d{4}\.\d{2}\.\d{2}|\d+[ë¶„ì‹œì¼ì£¼ì´ˆ]\s?ì „|ë°©ê¸ˆ\s?ì „', txt):
                                date_text = txt

                    # -- 3. ë‚ ì§œ í•„í„°ë§ ë¡œì§ (í•µì‹¬) --
                    # ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜
                    article_date_obj = parse_date_text(date_text)
                    
                    if article_date_obj:
                        # ê¸°ì‚¬ ë‚ ì§œê°€ ì‹œì‘ì¼ë³´ë‹¤ ê³¼ê±°ë¼ë©´ -> ìˆ˜ì§‘ ì¢…ë£Œ (ìµœì‹ ìˆœ ì •ë ¬ì´ë¯€ë¡œ)
                        if article_date_obj < start_d:
                            stop_crawling = True
                            break # í˜ì´ì§€ ë£¨í”„ íƒˆì¶œìš©
                        # ê¸°ì‚¬ ë‚ ì§œê°€ ì¢…ë£Œì¼ë³´ë‹¤ ë¯¸ë˜ë¼ë©´ -> ê±´ë„ˆë›°ê¸° (ì„¤ë§ˆ ë¯¸ë˜ ê¸°ì‚¬ê°€?)
                        if article_date_obj > end_d:
                            continue
                    
                    # -- 4. ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ í™•ì¸ --
                    final_link = original_link
                    is_naver = "n.news.naver.com" in original_link
                    
                    naver_btn = card.select_one('a.info[href*="n.news.naver.com"]')
                    if naver_btn:
                        final_link = naver_btn.get('href')
                        is_naver = True

                    # -- 5. ì¤‘ë³µ ì œê±° ë° ì¶”ê°€ --
                    if final_link in seen_links: continue
                    seen_links.add(final_link)
                    
                    full_title = f"{title}{paper_info}"
                    
                    all_results.append({
                        'title': full_title,
                        'link': final_link,
                        'press': press_name,
                        'is_naver': is_naver,
                        'is_paper': is_paper,
                        'date': date_text
                    })
                
                if stop_crawling:
                    with log_container: st.info("â„¹ï¸ ì„¤ì •ëœ ê¸°ê°„(ì‹œì‘ì¼) ì´ì „ì˜ ê¸°ì‚¬ê°€ ë°œê²¬ë˜ì–´ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                    
                time.sleep(0.5) # ì°¨ë‹¨ ë°©ì§€ ë”œë ˆì´
                
            except Exception as e:
                with log_container: st.error(f"Error on page {page}: {e}")
                continue
        
        progress_bar.progress(1.0)
        status_text.success(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_results)}ê±´")
        time.sleep(1.0)
        progress_bar.empty()
        status_text.empty()
        
        return all_results

# --- [4. UI ì„¤ì •] ---
st.set_page_config(page_title="Totta Scraper", layout="wide")

st.markdown("""
    <style>
    div[data-testid="column"] { 
        display: flex !important; 
        flex-direction: column !important; 
        justify-content: center !important; 
    }
    .news-card { 
        padding: 12px 16px; 
        border-radius: 8px; 
        border-left: 5px solid #007bff; 
        box-shadow: 0 2px 4px rgba(0,0,0,0.08); 
        background: white;
        height: 100%;
        display: flex;
        flex-direction: column;
        justify-content: center;
        margin-right: 15px !important;
        margin-bottom: 5px !important; 
    }
    .bg-scraped { background: #f8f9fa !important; border-left: 5px solid #adb5bd !important; opacity: 0.7; }
    .news-title { font-size: 16px !important; font-weight: 700; color: #222; margin-bottom: 5px; line-height: 1.3; }
    .news-meta { font-size: 13px !important; color: #666; font-weight: 500; }
    
    .stButton > button, .stLinkButton > a { 
        width: 100% !important; 
        height: 36px !important; 
        border-radius: 6px !important; 
        font-size: 13px !important; 
        font-weight: 600 !important; 
        padding: 0px 5px !important; 
        border: 1px solid #e0e0e0 !important; 
        background-color: white !important; 
        color: #555 !important; 
        box-shadow: 0 1px 2px rgba(0,0,0,0.05) !important; 
    }
    .stButton > button:hover, .stLinkButton > a:hover {
        border-color: #007bff !important; 
        color: #007bff !important; 
        background-color: #f0f7ff !important; 
    }
    .section-header { font-size: 18px; font-weight: 700; color: #333; margin-top: 30px; margin-bottom: 15px; border-bottom: 2px solid #007bff; display: inline-block; }
    
    div[data-testid="stHorizontalBlock"] .stButton:nth-of-type(2) button { color: #0056b3 !important; }
    div[data-testid="stHorizontalBlock"] .stButton:nth-of-type(3) button { color: #198754 !important; }
    </style>
    """, unsafe_allow_html=True)

# ì„¸ì…˜ ì´ˆê¸°í™”
for key in ['corp_list', 'rel_list', 'search_results']:
    if key not in st.session_state: st.session_state[key] = []

st.title("ğŸš‡ ë˜íƒ€ ìŠ¤í¬ë¦½í„° (Final Ver)")

# 1. ìŠ¤í¬ë© ëª©ë¡
t_date = get_target_date()
date_header = f"<{t_date.month}ì›” {t_date.day}ì¼ ì¡°ê°„ ìŠ¤í¬ë©>"

if st.session_state.corp_list or st.session_state.rel_list:
    final_output = f"{date_header}\n\n[ê³µì‚¬ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.corp_list) + "\n[ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.rel_list)
else:
    final_output = ""

text_height = max(180, (final_output.count('\n') + 1) * 25)
st.text_area("ğŸ“‹ ìŠ¤í¬ë© ê²°ê³¼", value=final_output, height=text_height)

if final_output:
    js_code = f"""
    <html>
        <head>
            <style>
                .copy-btn {{
                    display: inline-flex; align-items: center; justify-content: center;
                    width: 100%; height: 38px; background-color: #f0f2f6;
                    color: #31333F; border: 1px solid #d1d5db; border-radius: 6px;
                    cursor: pointer; font-family: sans-serif; font-weight: 600; font-size: 14px;
                }}
                .copy-btn:hover {{ border-color: #007bff; color: #007bff; background-color: #e7f3ff; }}
            </style>
        </head>
        <body>
            <textarea id="hidden-text" style="position:absolute; top:-9999px; left:-9999px;">{final_output}</textarea>
            <button class="copy-btn" onclick="copyToClipboard()">ğŸ“‹ í…ìŠ¤íŠ¸ ë³µì‚¬í•˜ê¸° (í´ë¦­)</button>
            <script>
                function copyToClipboard() {{
                    var textArea = document.getElementById("hidden-text");
                    textArea.select();
                    textArea.setSelectionRange(0, 99999);
                    try {{
                        var successful = document.execCommand('copy');
                        if (successful) alert('âœ… ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!');
                        else alert('âŒ ë³µì‚¬ ì‹¤íŒ¨.');
                    }} catch (err) {{ alert('âŒ ë¸Œë¼ìš°ì € ì°¨ë‹¨ë¨.'); }}
                }}
            </script>
        </body>
    </html>
    """
    components.html(js_code, height=50)

if st.button("ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”", use_container_width=True):
    st.session_state.corp_list, st.session_state.rel_list = [], []
    st.rerun()

with st.expander("ğŸ› ï¸ ìŠ¤í¬ë© í•­ëª© ê´€ë¦¬", expanded=False):
    st.write("**ğŸ¢ ê³µì‚¬ ë³´ë„**")
    for idx, item in enumerate(st.session_state.corp_list):
        ct, cd = st.columns([0.85, 0.15])
        with ct: st.caption(item.split('\n')[0])
        with cd: 
            if st.button("ì‚­ì œ", key=f"d_c_{idx}"): st.session_state.corp_list.pop(idx); st.rerun()
    st.write("**ğŸš† ìœ ê´€ê¸°ê´€ ë³´ë„**")
    for idx, item in enumerate(st.session_state.rel_list):
        ct, cd = st.columns([0.85, 0.15])
        with ct: st.caption(item.split('\n')[0])
        with cd:
            if st.button("ì‚­ì œ", key=f"d_r_{idx}"): st.session_state.rel_list.pop(idx); st.rerun()

st.divider()

# 2. ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì„¤ì •", expanded=True):
    keyword = st.text_input("ê²€ìƒ‰ì–´", value="ì„œìš¸êµí†µê³µì‚¬")
    d1, d2 = st.columns(2)
    with d1: start_d = st.date_input("ì‹œì‘ì¼", datetime.date.today())
    with d2: end_d = st.date_input("ì¢…ë£Œì¼", datetime.date.today())
    max_a = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 10, 100, 30)
    
    if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True):
        st.session_state.search_results = NewsScraper().fetch_news(start_d, end_d, keyword, max_a)

# 3. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
def display_list(title, items, key_prefix):
    st.markdown(f'<div class="section-header">{title} ({len(items)}ê±´)</div>', unsafe_allow_html=True)
    if not items:
        st.caption("ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    for i, res in enumerate(items):
        date_val = res.get('date', '')
        date_str = f"[{date_val}] " if date_val else ""
        item_txt = f"ã…‡ {date_str}{res['title']}_{res['press']}\n{res['link']}\n\n"
        
        is_scraped = (item_txt in st.session_state.corp_list) or (item_txt in st.session_state.rel_list)
        bg = "bg-scraped" if is_scraped else "bg-white"

        main_cols = st.columns([0.75, 0.25], gap="small")
        with main_cols[0]:
            st.markdown(f'''<div class="news-card {bg}">
                <div class="news-title">{res["title"]}</div>
                <div class="news-meta">
                    <span style="color: #007bff; font-weight: bold;">{date_val}</span>
                    [{res["press"]}] {"(ìŠ¤í¬ë©ë¨)" if is_scraped else ""}
                </div>
            </div>''', unsafe_allow_html=True)

        with main_cols[1]:
            btn_cols = st.columns(3, gap="small") 
            with btn_cols[0]:
                st.link_button("ì›ë¬¸", res['link'], use_container_width=True)
            with btn_cols[1]:
                if st.button("ê³µì‚¬", key=f"c_{key_prefix}_{i}", use_container_width=True):
                    if item_txt not in st.session_state.corp_list:
                        st.session_state.corp_list.append(item_txt)
                        st.toast("ğŸ¢ ì¶”ê°€ë¨!", icon="âœ…"); time.sleep(1.0); st.rerun()
            with btn_cols[2]:
                if st.button("ìœ ê´€", key=f"r_{key_prefix}_{i}", use_container_width=True):
                    if item_txt not in st.session_state.rel_list:
                        st.session_state.rel_list.append(item_txt)
                        st.toast("ğŸš† ì¶”ê°€ë¨!", icon="âœ…"); time.sleep(1.0); st.rerun()
        
        st.markdown("<div style='margin-bottom: 6px;'></div>", unsafe_allow_html=True)

# ê²°ê³¼ ë¶„ë¥˜ ë¡œì§
if st.session_state.search_results:
    paper_news = [x for x in st.session_state.search_results if x.get('is_paper')]
    naver_news = [x for x in st.session_state.search_results if x.get('is_naver') and not x.get('is_paper')]
    other_news = [x for x in st.session_state.search_results if not x.get('is_naver') and not x.get('is_paper')]
    
    if paper_news:
        display_list("ğŸ“° ì§€ë©´ ë³´ë„", paper_news, "p")
        st.write("") 
        
    display_list("ğŸŸ¢ ë„¤ì´ë²„ ë‰´ìŠ¤", naver_news, "n")
    st.write("")
    display_list("ğŸŒ ì–¸ë¡ ì‚¬ ìì²´ ê¸°ì‚¬", other_news, "o")
