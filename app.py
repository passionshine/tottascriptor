import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
import datetime
import time
import streamlit.components.v1 as components

# --- [1. ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜] ---
def get_target_date():
    today = datetime.date.today()
    if today.weekday() == 4: target = today + datetime.timedelta(days=3)
    elif today.weekday() == 5: target = today + datetime.timedelta(days=2)
    else: target = today + datetime.timedelta(days=1)

    holidays = [
        datetime.date(2025,1,1), datetime.date(2025,1,28), datetime.date(2025,1,29), datetime.date(2025,1,30),
        datetime.date(2025,3,1), datetime.date(2025,3,3), datetime.date(2025,5,5), datetime.date(2025,5,6),
        datetime.date(2025,6,6), datetime.date(2025,8,15), datetime.date(2025,10,3), datetime.date(2025,10,5),
        datetime.date(2025,10,6), datetime.date(2025,10,7), datetime.date(2025,10,8), datetime.date(2025,10,9), datetime.date(2025,12,25),
    ]
    while target in holidays or target.weekday() >= 5:
        target += datetime.timedelta(days=1)
    return target

# --- [2. ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼] ---
class NewsScraper:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/'
        }

    def fetch_news(self, start_d, end_d, keyword, max_articles):
        ds, de = start_d.strftime("%Y.%m.%d"), end_d.strftime("%Y.%m.%d")
        nso = f"so:dd,p:from{start_d.strftime('%Y%m%d')}to{end_d.strftime('%Y%m%d')}"
        all_results, seen_links = [], set()
        query = f'"{keyword}"'

        # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ì— ë§ì¶˜ í˜ì´ì§€ ê³„ì‚° (í˜ì´ì§€ë‹¹ ì•½ 10ê±´)
        max_pages = (max_articles // 10) + 1
        
        for page in range(max_pages):
            if len(all_results) >= max_articles: break
            start_val = (page * 10) + 1
            url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_pge&sort=1&photo=0&pd=3&ds={ds}&de={de}&nso={nso}&start={start_val}"
            
            try:
                res = self.scraper.get(url, headers=self.headers, timeout=10)
                soup = BeautifulSoup(res.content, 'html.parser')
                items = soup.select('a[data-heatmap-target=".tit"]')
                if not items: break

                for t_tag in items:
                    if len(all_results) >= max_articles: break
                    title, link = t_tag.get_text(strip=True), t_tag.get('href')
                    if link in seen_links: continue
                    
                    card = None
                    curr = t_tag
                    for _ in range(5):
                        if curr.parent:
                            curr = curr.parent
                            if curr.select_one(".sds-comps-profile") or curr.select_one(".news_info"):
                                card = curr; break
                    
                    press_name, date_text, is_naver = "ì•Œ ìˆ˜ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "n.news.naver.com" in link
                    if card:
                        naver_btn = card.select_one('a[href*="n.news.naver.com"]')
                        if naver_btn: link, is_naver = naver_btn.get('href'), True
                        p_el = card.select_one(".sds-comps-profile-info-title-text, .press_name, .info.press")
                        if p_el: press_name = p_el.get_text(strip=True)
                        t_el = card.select_one(".sds-comps-profile-info-subtexts, .news_info")
                        if t_el:
                            for txt in t_el.stripped_strings:
                                if ('ì „' in txt and len(txt) < 15) or ('.' in txt and len(txt) < 15 and txt[0].isdigit()):
                                    date_text = txt; break
                    
                    seen_links.add(link)
                    all_results.append({'title': title, 'link': link, 'press': press_name, 'time': date_text, 'is_naver': is_naver})
                time.sleep(0.2)
            except: break
        return all_results

# --- [3. UI ì„¤ì •] ---
st.set_page_config(page_title="ì„œìš¸êµí†µê³µì‚¬ ìŠ¤í¬ë©", layout="wide")

st.markdown("""
    <style>
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button, .stLinkButton > a {
        width: 100% !important; height: 32px !important;
        font-size: 11px !important; font-weight: 600 !important;
        padding: 0px 2px !important; border-radius: 6px !important;
        display: inline-flex !important; align-items: center !important;
        justify-content: center !important; white-space: nowrap !important;
    }
    /* ì¹´ë“œ ë° ì œëª© ìŠ¤íƒ€ì¼ */
    .news-card {
        background: white; padding: 12px; border-radius: 10px;
        border-left: 5px solid #007bff; margin-bottom: 5px;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
    }
    .news-title { 
        font-size: 15px !important; font-weight: 700; color: #111; 
        line-height: 1.4; word-break: keep-all; white-space: normal !important;
    }
    .news-meta { font-size: 12px !important; color: #777; margin-top: 4px; }
    [data-testid="column"] { padding: 0 3px !important; }
    </style>
    """, unsafe_allow_html=True)

for key in ['corp_list', 'rel_list', 'search_results']:
    if key not in st.session_state: st.session_state[key] = []

t_date = get_target_date()
date_header = f"<{t_date.month}ì›” {t_date.day}ì¼({['ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','í† ','ì¼'][t_date.weekday()]}) ì¡°ê°„ ìŠ¤í¬ë©>"

st.title("ğŸš‡ ì¡°ê°„ ë‰´ìŠ¤ ìŠ¤í¬ë©")

# 1. ê²°ê³¼ ìƒì
final_output = f"{date_header}\n\n[ê³µì‚¬ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.corp_list) + "\n[ì² ë„ ë“± ê¸°íƒ€ ìœ ê´€ê¸°ê´€ ê´€ë ¨ ë³´ë„]\n" + "".join(st.session_state.rel_list)
st.text_area("ğŸ“‹ ìŠ¤í¬ë© ê²°ê³¼", value=final_output, height=180)

if st.button("ğŸ“‹ ì „ì²´ ë³µì‚¬í•˜ê¸°"):
    st.toast("âœ… í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
    components.html(f"<script>navigator.clipboard.writeText(`{final_output}`);</script>", height=0)

st.divider()

# 2. ê²€ìƒ‰ ì„¤ì •
with st.expander("ğŸ” ê²€ìƒ‰ í•„í„° ë° ìˆ˜ì§‘ ì„¤ì •", expanded=True):
    keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", value="ì„œìš¸êµí†µê³µì‚¬")
    c1, c2 = st.columns(2)
    with c1: start_d = st.date_input("ì‹œì‘ì¼", datetime.date.today()-datetime.timedelta(days=1))
    with c2: end_d = st.date_input("ì¢…ë£Œì¼", datetime.date.today())
    
    # [ë³€ê²½] ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ìŠ¬ë¼ì´ë”
    max_a = st.slider("ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", min_value=10, max_value=100, value=30, step=10)
    
    # [ì¶”ê°€] í•„í„° ê°œìˆ˜ ê³„ì‚°
    all_res = st.session_state.search_results
    n_count = len([r for r in all_res if r['is_naver']])
    p_count = len([r for r in all_res if not r['is_naver']])
    
    filter_choice = st.radio(
        "ë³´ê¸° í•„í„°", 
        [f"ëª¨ë‘ ë³´ê¸° ({len(all_res)})", f"ë„¤ì´ë²„ ê¸°ì‚¬ ({n_count})", f"ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬ ({p_count})"], 
        horizontal=True
    )

if st.button("ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary"):
    st.session_state.search_results = []
    with st.spinner('ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...'):
        results = NewsScraper().fetch_news(start_d, end_d, keyword, max_a)
        st.session_state.search_results = results
        st.rerun()

# 3. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
if st.session_state.search_results:
    # í•„í„°ë§ ë¡œì§
    if "ë„¤ì´ë²„ ê¸°ì‚¬" in filter_choice:
        display_results = [r for r in st.session_state.search_results if r['is_naver']]
    elif "ì–¸ë¡ ì‚¬ ìì²´ê¸°ì‚¬" in filter_choice:
        display_results = [r for r in st.session_state.search_results if not r['is_naver']]
    else:
        display_results = st.session_state.search_results

    for i, res in enumerate(display_results):
        with st.container():
            # [ë³€ê²½] ì œëª© ëì— ì›ë¬¸ë³´ê¸° ë²„íŠ¼ ë°°ì¹˜
            t_col, b_col = st.columns([0.82, 0.18])
            with t_col:
                st.markdown(f"""
                <div class="news-card">
                    <div class="news-title">{res['title']}</div>
                    <div class="news-meta">[{res['press']}] {res['time']}</div>
                </div>
                """, unsafe_allow_html=True)
            with b_col:
                st.write("") # ì œëª© ë†’ì´ ë§ì¶¤ìš©
                st.link_button("ğŸ”— ì›ë¬¸ë³´ê¸°", res['link'])
            
            # í•˜ë‹¨ ìŠ¤í¬ë© ë²„íŠ¼ 2ê°œ
            s1, s2 = st.columns(2)
            with s1:
                if st.button(f"ğŸ¢ ê³µì‚¬ ë³´ë„ +", key=f"c_{i}"):
                    item = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
                    if item not in st.session_state.corp_list:
                        st.session_state.corp_list.append(item)
                        st.toast(f"âœ… ê³µì‚¬ ì„¹ì…˜ì— ì¶”ê°€ë¨!", icon="ğŸ¢")
                        st.rerun()
            with s2:
                if st.button(f"ğŸš† ìœ ê´€ê¸°ê´€ ë³´ë„ +", key=f"r_{i}"):
                    item = f"ã…‡ {res['title']}_{res['press']}\n{res['link']}\n\n"
                    if item not in st.session_state.rel_list:
                        st.session_state.rel_list.append(item)
                        st.toast(f"âœ… ìœ ê´€ê¸°ê´€ ì„¹ì…˜ì— ì¶”ê°€ë¨!", icon="ğŸš†")
                        st.rerun()
        st.write("---")